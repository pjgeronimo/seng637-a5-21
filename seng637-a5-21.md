**SENG 637- Dependability and Reliability of Software Systems\***

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:      | 21  |
| -------------- | --- |
| Student Names: |     |
| Carrie         |     |
| Jon            |     |
| Paolo          |     |
| Israel         |     |
| Henry          |     |

# Introduction

The goal of this lab is to explore the ways to assess failure data including reliability growth testing (part 1) and assessment using a reliability demonstration chart (part 2)

For part 1 this involves using a growth assessment tool. For this lab, the tool selected was C-SFRAT as for its user-friendly interface and easy setup. With this tool, the given test data will be imported to assess various models, and to analyze failure rate, MTTF and reliability metrics.

In part 2, a reliability demonstration chart tool will be used to determine if the target failure or MTTF criteria is met. The tool will give insight to the reliability trend of the SUT.

These two methods will be discussed and compared for their effectiveness as reliability assessment tools.

#

# Assessment Using Reliability Growth Testing

When testing the full dataset, each model/hazard method in C-SFRAT was applied to see which would provide the best fit to the data. This included the following:

- IFR Salvia & Bollinger, IFR generalized Salvia & Bollinger
- S Distribution
- Discrete Weibull (Order 2), Discrete Weibull (Type III)
- Geometric
- Negative Binomial (Order 2)
- Truncated Logistic

These Each model was also tested with varying covariates between E, F and C. This allows the variable to be predictors in the model to adjust the estimates of the primary effect. 57 combination were tested and the dataset was exported to a csv [model_results_group21](./model_results_group21.csv).

From the dataset, the ranking of the best 2 models were determined by considering the metric weights giving slightly more weight to the BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion) metrics. This is due to the nature of these methods that penalize overfitting, preventing unnecessary complexity. They are a metric that gives insight into the goodness of fit although have slight differences in their methodology. A lower value for either metric signifies a better fit. Other metrics were considered as well, such as Error Sum of Squares (SSE) to ensure these models had good performance across the various metrics.

In the dataset for all models, the top 2 both had the lowest scores for BIC and AIC and was determined by considering the critic method, which can rank based on the weighted combination of the metrics. The top 2 models can be seen below. These were 1. DW3 model with F covariate. and 2. Geometric with F covariate.

![top2table](/screenshots/top_2_table.png?raw=true)

The plot of time to failures plot for the top 2 models against the imported failure data can be seen below.

![top2plot](/screenshots/top_2_plot.png?raw=true)

When considering the range analysis of the dataset, subsets were tested sequentially to assess at which point, the model could continue to make relatively accurate predictions on the test data. It was observed that after including some points after the slope increase (around interval 19-22) provided enough information to the model to continue predicting accurately. The good fit that allowed to test against adequate testing points seemed to be using interval 0-22, which was about 70% of the training data. The remaining 20% (interval 23-31) was used to see if the model could continue predicting well.

The below image shows the top 2 models predicting the remaining 9 intervals of the testing data (interval 23-31) where the GM model is in orange and DW3 in blue. The best results came from adding more effort per interval to the F covariate.

![top2plot](/screenshots/top_2_predict.png?raw=true)

The intensity plot for the sub-interval can be seen below. The sofware was unable to show the predicted intensity for the remaining points on the plot. Those were manually calculated using the data from the table rather than the plot.

![top2plot](/screenshots/top_2_intensity.png?raw=true)

The failure rate was then determined using the full data to compare against the failure rate of the predictions from the DW3 and GM models.

| Dataset       | Failure Rate (F/interval) | MTTF (intervals) |
| ------------- | ------------------------- | ---------------- |
| original data | 92/31 = 2.96              | 1/2.96 = 0.337   |
| DW3 Predict   | 88/31 = 2.83              | 1/2.83 = 0.353   |
| GM Predict    | 92/21 = 2.96              | 1/2.96 = 0.337   |

The resulting failure rate from the original data and the predictions of the top 2 models are relatively close across the whole interval, suggesting the models were a good fit and continued to predict well across the remaining intervals.

### Decision making for target failure rate

Given the results, a company may base future performance against a set failure rate based on historical performance, similar to this lab. They can then use models to predict if the failure rate may become unacceptable in the future.

In this lab, if the failure data was an accurate representation of the company's operational conditions, a failure rate of 3 may be a good target value to maintain.

### Advantages and disadvantages of reliability growth testing

Some advantages include the predicting ability, allowing organizations to make informed decisions or allocate resources differently if needed. It can also be advantageous to identify problematic areas such as surges in the systems reliability and failures.

The disadvantages include the difficulty to actually predict reliability. Results may be different and there are often many external variables that may alter and influcence the failures. Additionally, this analysis requires adequate collected data on the system's reliability. This is not always available, accurate or easy to obtain. This may be a hurdle for organizations to correctly implement.

# Assessment Using Reliability Demonstration Chart

## Data Reformatting

Since the given data describes failure count at individual time intervals, the data needs to be reformatted before plotting it on the RDC. Assuming that the failures are evenly spread throughout the time interval they take place in, the time period for each individual failure can be determined. Therefore the cumulative time for each failure can also be determined, giving us the "timestamp" at which each failure occured.

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

# How the team work/effort was divided and managed

#

# Difficulties encountered, challenges overcome, and lessons learned

# Comments/feedback on the lab itself
