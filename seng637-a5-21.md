**SENG 637- Dependability and Reliability of Software Systems\***

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:      | 21  |
| -------------- | --- |
| Student Names: |     |
| Carrie         |     |
| Jon            |     |
| Paolo          |     |
| Israel         |     |
| Henry          |     |

# Introduction

The goal of this lab is to explore the ways to assess failure data including reliability growth testing (part 1) and assessment using a reliability demonstration chart (part 2)

For part 1 this involves using a growth assessment tool. For this lab, the tool selected was C-SFRAT as for its user-friendly interface and easy setup. With this tool, the given test data will be imported to assess various models, and to analyze failure rate, MTTF and reliability metrics.

In part 2, a reliability demonstration chart tool will be used to determine if the target failure or MTTF criteria is met. The tool will give insight to the reliability trend of the SUT.

These two methods will be discussed and compared for their effectiveness as reliability assessment tools.

#

# Assessment Using Reliability Growth Testing

When testing the full dataset, each model/hazard method in C-SFRAT was applied to see which would provide the best fit to the data. This included the following:

- IFR Salvia & Bollinger, IFR generalized Salvia & Bollinger
- S Distribution
- Discrete Weibull (Order 2), Discrete Weibull (Type III)
- Geometric
- Negative Binomial (Order 2)
- Truncated Logistic

These Each model was also tested with varying covariates between E, F and C. This allows the variable to be predictors in the model to adjust the estimates of the primary effect. The full comparison of each model across the entire dataset was exported to the csv [Model Results Group 21](./model_results_group21.csv)
.

From the dataset, the ranking of the best 2 models were determined by putting more consideration on the BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion) metrics. This is due to the nature of these methods that penalize overfitting, preventing unnecessary complexity. They both provide a goodness of fit although have slight differences in their methodology. A lower value for either metric signifies a better fit.

In the dataset for all models, the top 2 both had the lowest scores for BIC and AIC and can be seen in the figure below. These were 1. DW3 model with F covariate. and 2. GMOther metrics were considered as well, such as Error Sum of Squares (SSE) to ensure these models had good performance across various metrics.

![top2table](/screenshots/top_2_table.png?raw=true)

The plot of failures vs. intervals for the top 2 models with the imported failure data can be seen below.

![top2plot](/screenshots/top_2_plot.png?raw=true)

JF - add intensity plot, talk about sub-intervals , and acceptable error

# Assessment Using Reliability Demonstration Chart

#

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

# How the team work/effort was divided and managed

#

# Difficulties encountered, challenges overcome, and lessons learned

# Comments/feedback on the lab itself
